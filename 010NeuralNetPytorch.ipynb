{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2a2808",
   "metadata": {},
   "source": [
    "## Pytorch Fundamentals\n",
    "- Tensor: Core data structure\n",
    "    - Multidimensional array\n",
    "    - Has shape and a data type\n",
    "    - Can live one the GPU\n",
    "    - Supports auto differentiation\n",
    "\n",
    "- Inputs and outputs are tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcfc85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3b8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4., 7.],\n",
      "        [2., 3., 6.]])\n",
      "Size:  torch.Size([2, 3])\n",
      "Shape:  torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor\n",
    "X = torch.tensor([[1.0,4.0,7.0],[2.0,3.0,6.0]])\n",
    "print(X)\n",
    "\n",
    "# Properties\n",
    "print(\"Size: \", X.shape)\n",
    "print(\"Shape: \", X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "024af58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "tensor([4., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Indexing\n",
    "print(X[0,1])\n",
    "print(X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67412a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arithmatic: \n",
      " tensor([[20., 50., 80.],\n",
      "        [30., 40., 70.]])\n",
      "Itemwise Exponential: \n",
      " tensor([[   2.7183,   54.5982, 1096.6332],\n",
      "        [   7.3891,   20.0855,  403.4288]])\n",
      "Mean: \n",
      " tensor(3.8333)\n",
      "Max dim per column: \n",
      " torch.return_types.max(\n",
      "values=tensor([2., 4., 7.]),\n",
      "indices=tensor([1, 0, 0]))\n",
      "Maxrix Transpose + Multiplication\n",
      " tensor([[66., 56.],\n",
      "        [56., 49.]])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "APIs for computations\n",
    "'''\n",
    "# Itemwise addition and multiplication\n",
    "print(\"Arithmatic: \\n\", 10 * (X + 1.0))\n",
    "print(\"Itemwise Exponential: \\n\", X.exp())\n",
    "print(\"Mean: \\n\", X.mean())\n",
    "print(\"Max dim per column: \\n\", X.max(dim=0))\n",
    "print(\"Maxrix Transpose + Multiplication\\n\",\n",
    "      X @ X.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd7888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 4. 7.]\n",
      " [2. 3. 6.]] float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.],\n",
       "        [2., 3., 6.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Converting tensor to np array and vice versa\n",
    "\n",
    "Notice:\n",
    "    Default precision is 32bit in pytorch,\n",
    "    64 bit in numpy\n",
    "    Deep learning likes 32 bits, half ram, speeds up\n",
    "    computation, and that neural nets do not need such precision\n",
    "'''\n",
    "import numpy as np\n",
    "print(X.numpy(), X.numpy().dtype)\n",
    "\n",
    "torch.tensor(np.array([[1., 4., 7.], [2., 3., 6.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e953caf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Do you have GPU acceleartion?\n",
    "'''\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49ad5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Set up GPU runtime\n",
    "'''\n",
    "M = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
    "M = M.to(device)\n",
    "\n",
    "M.device\n",
    "\n",
    "# OR do this:\n",
    "M = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88026bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14., 32.],\n",
       "        [32., 77.]], device='mps:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "After selecting Runtime, can do operations that now takes place of GPU\n",
    "'''\n",
    "R = M @ M.T \n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07f36921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "984 μs ± 9.8 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "944 μs ± 2.54 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "What is the actual speed difference?\n",
    "'''\n",
    "M = torch.rand((1000, 1000))  # On Cpu\n",
    "\n",
    "%timeit M @ M.T\n",
    "\n",
    "M = torch.rand((1000, 1000), device=device)\n",
    "%timeit M @ M.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05df3bc",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "Pytorch comes with reverse-mode auto-differentiation (automatic gradients)\n",
    "We know derivative of x^2 is 2x\n",
    "Lets evaulate it at f(5) and f'(5), we get 25 and 10 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc8dff06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25., grad_fn=<PowBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "1. Create tensor x = 5. requries_grad=True means that it is a\n",
    "    variable and not a constant\n",
    "    Pytorhc will automatically keep track of all operations involving x\n",
    "\n",
    "2. We compute f = x**2, resulting tensor is 25.0, square of 5.0\n",
    "    f also carries a grad_fn attribute, represents operation that\n",
    "        created this tensor\n",
    "\n",
    "3. Then call f.backward() which backpropagates the gradients\n",
    "    through the computation graph, starting at f and all the way back to\n",
    "    leaf nodes (just x in this case)\n",
    "\n",
    "\n",
    "4. Lastly, read x tensor's grad attribute, computed during backprop\n",
    "    Giving us derivative of f in reagards to x\n",
    "'''\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "f = x ** 2\n",
    "print(f)\n",
    "\n",
    "f.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ab7e796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "After computing gradients, you generally would want to perform a\n",
    "    gradient descent step by subrtacting a fraction of the gradients\n",
    "    from the model variables when training a neural net.\n",
    "\n",
    "In the example, running gradient descent will gradually push x to 0.\n",
    "\n",
    "To run grad descent, want to temporarily disable gradient tracking \n",
    "    as you don't want to track the descent step itself in computation graph.\n",
    "\n",
    "Can be done by doing torch.no_grad\n",
    "'''\n",
    "\n",
    "learning_rate = 0.1\n",
    "with torch.no_grad():\n",
    "    x -= learning_rate * x.grad\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "# or use variable's detach() method, which creates a new tensor.\n",
    "# points to the same data in memory, but detached from computation graph\n",
    "# Useful if you want fine-grained control over which operations\n",
    "# should contribute to gradient coputation\n",
    "'However using no_grad() is generally preferred when performing'\n",
    "'inference or descent step'\n",
    "x_detached = x.detach()\n",
    "x_detached -= learning_rate * x.grad\n",
    "print(x_detached)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6be17bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Finally, before repeating the whole process \n",
    "(forward & back pass, and gradient descent), essential to\n",
    "zero out gradients of very model parameter\n",
    "'''\n",
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fadad702",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Every together, whole training loop looks like:\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "for iteration in range(100):\n",
    "    f = x ** 2  # forward pass\n",
    "    f.backward()  # backward pass\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad  # gradient descent step\n",
    "\n",
    "    x.grad.zero_()  # reset the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d13e7c",
   "metadata": {},
   "source": [
    "**IMPORTANT**:\n",
    "In-place operations don't always play nicely with autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6270aeaa",
   "metadata": {},
   "source": [
    "## Implementing Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ce7cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "\n",
    "# Convert to tensor and normalize it\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_valid = torch.FloatTensor(X_valid)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "means = X_train.mean(dim=0, keepdims=True)\n",
    "stds = X_train.std(dim=0, keepdims=True)\n",
    "\n",
    "X_train = (X_train - means) / stds\n",
    "X_valid = (X_valid - means) / stds \n",
    "X_test = (X_test - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert targets to tensors too\n",
    "# Since predictions are column vectors, we need to ensure target are\n",
    "# also column vectors\n",
    "# However, numpy arrays represents the target are one-dimensions,\n",
    "# need to rehape the tensors to column vecors by adding second dim size 1\n",
    "\n",
    "y_train = torch.FloatTensor(y_train).reshape(-1,1)\n",
    "y_valid = torch.FloatTensor(y_valid).reshape(-1, 1)\n",
    "y_test = torch.FloatTensor(y_test).reshape(-1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
